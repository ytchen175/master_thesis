{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ch</th>\n",
       "      <th>ch_SC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>而在中時報系以不堪虧損為由捨棄晚報的同時，另方面卻持續入股中天電視台，並有意在未來收購中視，...</td>\n",
       "      <td>而在中时报系以不堪亏损为由舍弃晚报的同时，另方面却持续入股中天电视台，并有意在未来收购中视，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>終戰後的十餘年間，可說是歌仔戲的黃金時代，人才輩出，除了活躍於戲院舞台的「內台戲」外，還有「...</td>\n",
       "      <td>终战后的十余年间，可说是歌仔戏的黄金时代，人才辈出，除了活跃於戏院舞台的「内台戏」外，还有「...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>「國民美術」以非學院派美術基調的發展過程，巧妙地與 1998 年國內社區大學興起的「解放知識...</td>\n",
       "      <td>「国民美术」以非学院派美术基调的发展过程，巧妙地与 1998 年国内社区大学兴起的「解放知识...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>汐止社大主任潘英海表示：「國民美術像集體參與的美術豐年祭，劉秀美在社區成立畫會，建構集體記憶...</td>\n",
       "      <td>汐止社大主任潘英海表示：「国民美术像集体参与的美术丰年祭，刘秀美在社区成立画会，建构集体记忆...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>例如日治時期祖父在金瓜石經營「鈔利搗礦場」的鄭炯輝，將當時用水車淘洗金砂的過程，一一用圖畫記...</td>\n",
       "      <td>例如日治时期祖父在金瓜石经营「钞利捣矿场」的郑炯辉，将当时用水车淘洗金砂的过程，一一用图画记...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310911</th>\n",
       "      <td>誰來對抗資本主義？</td>\n",
       "      <td>谁来对抗资本主义？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310912</th>\n",
       "      <td>在西畫荒漠中披荊斬棘</td>\n",
       "      <td>在西画荒漠中披荆斩棘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310913</th>\n",
       "      <td>我怕！」</td>\n",
       "      <td>我怕！」</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310914</th>\n",
       "      <td>多麼主觀傲慢！</td>\n",
       "      <td>多么主观傲慢！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310915</th>\n",
       "      <td>不只是給一條魚﹘﹘中華民國駐史瓦濟蘭技術服務</td>\n",
       "      <td>不只是给一条鱼﹘﹘中华民国驻史瓦济兰技术服务</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310916 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       ch  \\\n",
       "0       而在中時報系以不堪虧損為由捨棄晚報的同時，另方面卻持續入股中天電視台，並有意在未來收購中視，...   \n",
       "1       終戰後的十餘年間，可說是歌仔戲的黃金時代，人才輩出，除了活躍於戲院舞台的「內台戲」外，還有「...   \n",
       "2       「國民美術」以非學院派美術基調的發展過程，巧妙地與 1998 年國內社區大學興起的「解放知識...   \n",
       "3       汐止社大主任潘英海表示：「國民美術像集體參與的美術豐年祭，劉秀美在社區成立畫會，建構集體記憶...   \n",
       "4       例如日治時期祖父在金瓜石經營「鈔利搗礦場」的鄭炯輝，將當時用水車淘洗金砂的過程，一一用圖畫記...   \n",
       "...                                                   ...   \n",
       "310911                                          誰來對抗資本主義？   \n",
       "310912                                         在西畫荒漠中披荊斬棘   \n",
       "310913                                               我怕！」   \n",
       "310914                                            多麼主觀傲慢！   \n",
       "310915                             不只是給一條魚﹘﹘中華民國駐史瓦濟蘭技術服務   \n",
       "\n",
       "                                                    ch_SC  \n",
       "0       而在中时报系以不堪亏损为由舍弃晚报的同时，另方面却持续入股中天电视台，并有意在未来收购中视，...  \n",
       "1       终战后的十余年间，可说是歌仔戏的黄金时代，人才辈出，除了活跃於戏院舞台的「内台戏」外，还有「...  \n",
       "2       「国民美术」以非学院派美术基调的发展过程，巧妙地与 1998 年国内社区大学兴起的「解放知识...  \n",
       "3       汐止社大主任潘英海表示：「国民美术像集体参与的美术丰年祭，刘秀美在社区成立画会，建构集体记忆...  \n",
       "4       例如日治时期祖父在金瓜石经营「钞利捣矿场」的郑炯辉，将当时用水车淘洗金砂的过程，一一用图画记...  \n",
       "...                                                   ...  \n",
       "310911                                          谁来对抗资本主义？  \n",
       "310912                                         在西画荒漠中披荆斩棘  \n",
       "310913                                               我怕！」  \n",
       "310914                                            多么主观傲慢！  \n",
       "310915                             不只是给一条鱼﹘﹘中华民国驻史瓦济兰技术服务  \n",
       "\n",
       "[310916 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('test_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>簡體字</th>\n",
       "      <th>正體字</th>\n",
       "      <th>非對稱簡繁字</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>卜</td>\n",
       "      <td>['卜', '蔔']</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>叉</td>\n",
       "      <td>['叉']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一</td>\n",
       "      <td>['一']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>又</td>\n",
       "      <td>['又']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>口</td>\n",
       "      <td>['口']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>钻</td>\n",
       "      <td>['鑽']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>箩</td>\n",
       "      <td>['籮']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>銮</td>\n",
       "      <td>['鑾']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>蛮</td>\n",
       "      <td>['蠻']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>锣</td>\n",
       "      <td>['鑼']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4637 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     簡體字         正體字  非對稱簡繁字\n",
       "0      卜  ['卜', '蔔']    True\n",
       "1      叉       ['叉']   False\n",
       "2      一       ['一']   False\n",
       "3      又       ['又']   False\n",
       "4      口       ['口']   False\n",
       "...   ..         ...     ...\n",
       "4632   钻       ['鑽']   False\n",
       "4633   箩       ['籮']   False\n",
       "4634   銮       ['鑾']   False\n",
       "4635   蛮       ['蠻']   False\n",
       "4636   锣       ['鑼']   False\n",
       "\n",
       "[4637 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_to_tc_table = pd.read_csv('簡化字對照標準字總表.csv')\n",
    "sc_to_tc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/wirl/anaconda3/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wirl/anaconda3/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/wirl/anaconda3/lib/libcudart.so'), PosixPath('/home/wirl/anaconda3/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA SETUP: CUDA runtime path found: /home/wirl/anaconda3/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/wirl/anaconda3/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from peft import AutoPeftModelForCausalLM # PEFT = Parameter-Efficient Fine-Tuning\n",
    "\n",
    "model_name = \"/home/wirl/ytc/chinese-alpaca-2-7b_自己下載的\"\n",
    "\n",
    "def init_model(ft_num):\n",
    "    adapter_model_path = f\"/home/wirl/ytc/要寫的論文研究/code/qlora/output/checkpoint-{ft_num}\"\n",
    "    # merged_model_path = \"/home/wirl/ytc/要寫的論文研究/code/results/chinese-alpaca-2-7b_merged_model_by_qlora/\"\n",
    "\n",
    "    print(f\"Using ft_num={ft_num} model.\")\n",
    "\n",
    "    # https://github.com/artidoro/qlora/issues/29#issuecomment-1737072311\n",
    "    # 請放棄 merge model，乖乖用 AutoPeftModelForCausalLM 接住 qlora.py 訓練的 adapter\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        adapter_model_path,\n",
    "        local_files_only=True, \n",
    "        load_in_4bit=True, \n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_response(tokenizer, model, prompt_template, sentence_text, remove_input=True):\n",
    "    device = \"cuda:0\"\n",
    "    full_prompt = prompt_template.format(sentence_text)\n",
    "\n",
    "    # temperature, top_p, and top_k are only active when do_sample=True\n",
    "    # if you set Top-k to 10, the LLM will only consider the 10 most probable next words. \n",
    "    # This will result in more fluent text, but it will also reduce the diversity of the text. \n",
    "    # TOP_K = 30\n",
    "    TOP_K = 50\n",
    "    # TOP_K = 70\n",
    "    # If you set Top-p to 0.9, the LLM will only generate words that have a probability of at least 0.9. \n",
    "    # This will result in more diverse text, but it could also result in less fluent text.\n",
    "    TOP_P = 0.95\n",
    "    # TOP_P = 1.0\n",
    "\n",
    "    # TEMP = 1.0 # don't specify\n",
    "\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # outputs = model.generate(**inputs, max_new_tokens=len(sentence_text))\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=len(sentence_text), \n",
    "        do_sample=True, \n",
    "        # temperature=TEMP,\n",
    "        top_k=TOP_K, \n",
    "        top_p=TOP_P, \n",
    "        num_return_sequences=1,\n",
    "    ) # for translation, https://huggingface.co/docs/transformers/tasks/translation#inference\n",
    "\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if remove_input:\n",
    "        # 從 generate 出來的 output 中刪除 input text 的部分\n",
    "        cleaned_output = decoded_output.replace(full_prompt, \"\")\n",
    "        return cleaned_output\n",
    "    else:\n",
    "        return decoded_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ft_num=1250 model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3827d75f444ee99614e17e63791325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FT_NUM = 1250\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, local_files_only=True, legacy=True)\n",
    "model = init_model(FT_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "model.config.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 而在中時報系以不堪虧損為由捨棄週報同時，另方面卻持續入股中天電視臺，並有意在未來收購中視，成就跨媒體集團霸業。\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "sentence = df.loc[0, 'ch_SC']\n",
    "\n",
    "translate_prompt_template = \"\"\"\n",
    "### Instruction:\n",
    "翻譯成繁體中文: {}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(get_response(tokenizer, model, translate_prompt_template, sentence))\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "而在中時報系以不堪虧損為由捨棄晚報同時，另方面卻持續入股中天電視台，並有意在未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      " 而在中時報系以不堪虧損為由捨棄日報同時，另方面卻持續入股中天電視台，並有意於未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      "而在中時報系以不堪虧損為由捨棄週刊同時，另方面卻持續入股中天電視台，並有意在未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      "而在中時報系以不堪虧損為由捨棄週報同時，另方面卻持續入股中天電視臺，並有意在未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      "而在中時報系以不堪虧損為由捨棄週刊雜誌同時，另方面卻持續入股中天電視台，並有意在未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      "而在中時報系以不堪虧損為由捨棄晚報同時，另方面卻持續入股中天電視臺，並有意未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      " 而在中時報系以不堪虧損為由捨棄週刊雜誌同時，另方面卻持續入股中天電視臺，並有意於未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      "而在中時報系以不堪虧損為由捨棄週刊同時，另方面卻持續入股中天電視台，並有意未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      "而在中時報系以不堪虧損為由捨棄週報同時，另方面卻持續入股中天電視台，並有意在未來收購中視，成就跨媒體集團霸業。\n",
      "---\n",
      "而在中報系以不堪虧損為由捨棄週刊同時，另方面卻持續入股中天電視台，並有意於未來收購中視，成就跨媒體集團霸業。\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    print(get_response(tokenizer, model, translate_prompt_template, sentence))\n",
    "    print('---')\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"台中后里位在台湾南北的交会点，隐藏着许多全国知名的景点。\"\n",
    "sentence2 = \"我干什么不干你事。\"\n",
    "sentence3 = \"我发现太后的头发很干燥。\"\n",
    "sentence4 = \"芋头发芽了。\"\n",
    "sentence5 = \"再坐在电脑前面 我头发都没了T_T。\"\n",
    "sentence6 = \"他觉得丑时人生的通常都比较丑。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 臺中後里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      "臺中後里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      " 臺中後里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      " 臺中后里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      "臺中後里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      " 臺中後里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      " 臺中後里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      " 臺中后里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      " 臺中後里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n",
      " 臺中後里位在臺灣南北的交會點，隱藏著許多全國知名的景點。\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    print(get_response(tokenizer, model, translate_prompt_template, sentence1))\n",
    "    print('---')\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>簡體字</th>\n",
       "      <th>正體字</th>\n",
       "      <th>非對稱簡繁字</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>台</td>\n",
       "      <td>['台', '臺', '颱']</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>并</td>\n",
       "      <td>['并', '並', '併']</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>系</td>\n",
       "      <td>['系', '係', '繫']</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>舍</td>\n",
       "      <td>['舍', '捨']</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>面</td>\n",
       "      <td>['面', '麵']</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     簡體字              正體字  非對稱簡繁字\n",
       "174    台  ['台', '臺', '颱']    True\n",
       "355    并  ['并', '並', '併']    True\n",
       "629    系  ['系', '係', '繫']    True\n",
       "961    舍       ['舍', '捨']    True\n",
       "1388   面       ['面', '麵']    True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence = \"而在中时报系以不堪亏损为由舍弃晚报的同时，另方面却持续入股中天电视台，并有意在未来收购中视，成就跨媒体集团霸业。\"\n",
    "asymmetric_words_in_sentence = set(sc_to_tc_table[sc_to_tc_table['非對稱簡繁字'] == True]['簡體字']) & set(sentence)\n",
    "\n",
    "sc_to_tc_table[sc_to_tc_table['簡體字'].isin(asymmetric_words_in_sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[台]->[臺, 臺, 颱], 並->[並, 併, 併]，[係]->[係, 係]，[相]->[相, 相]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "e = \"而在中时报[系]以不堪亏损为由[舍]弃晚报的同时，另方[面]却持续入股中天电视[台]，[并]有意在未来收购中视，成就跨媒体集团霸业。\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### Instruction:\n",
    "被[]框住的字是非對稱簡繁字，你必須根據上下文來推斷該如何翻譯此字，可能的字在->後面的[]裡，\n",
    "[台]->[台, 臺, 颱], 并->[并, 並, 併], 系->[系, 係, 繫], 舍->[舍, 捨], 面->[面, 麵]\n",
    "翻譯成繁體中文: 而在中时报[系]以不堪亏损为由[舍]弃晚报的同时，另方[面]却持续入股中天电视[台]，[并]有意在未来收购中视，成就跨媒体集团霸业。\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(get_response(tokenizer, model, prompt_template, e))\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 在中時報[係, 係, 係]以不堪虧損為由[捨]->[捨, 捨, 捨]棄週報同時，另方[面]->[面, 麵, 面]卻持續入股中天電視[台]->[臺, \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "e = \"而在中时报[系]以不堪亏损为由[舍]弃晚报的同时，另方[面]却持续入股中天电视[台]，[并]有意在未来收购中视，成就跨媒体集团霸业。\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### Instruction:\n",
    "被[]框住的字是非對稱簡繁字，你必須根據上下文來推斷該如何翻譯此字，可能的字在->後面的[]裡，\n",
    "翻譯成繁體中文: 而在中时报[系][系, 係, 繫]以不堪亏损为由[舍]->[舍, 捨]弃晚报的同时，另方[面]->[面, 麵]却持续入股中天电视[台]->[台, 臺, 颱]，[并]->[并, 並, 併]有意在未来收购中视，成就跨媒体集团霸业。\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(get_response(tokenizer, model, prompt_template, e))\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "from rouge import Rouge\n",
    "import nltk.translate.gleu_score as gleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# 可參考\n",
    "src = list(jieba.cut(\"上層漏水耍手段不去處理可以怎麼做\"))\n",
    "ref = list(jieba.cut(\"上層漏水耍手段不去處理可以怎麼做\"))\n",
    "\n",
    "# ref: https://github.com/gcunhase/NLPMetrics\n",
    "\n",
    "score_gleu = gleu.sentence_gleu([ref], src, min_len=1, max_len=2)\n",
    "print(score_gleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk.translate.gleu_score as gleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "true_sentence = \"而在中時報系以不堪虧損為由捨棄晚報的同時，另方面卻持續入股中天電視台，並有意在未來收購中視，成就跨媒體集團霸業。\"\n",
    "revised_response = \"而在中時報系以不堪虧損為由捨棄週報雜誌的同時，另方面卻持續入股中天電視台，並有意在未來收購中視，成就跨媒體集團霸業。\"\n",
    "\n",
    "ref = list(jieba.cut(true_sentence))\n",
    "src = list(jieba.cut(revised_response))\n",
    "\n",
    "print('---')\n",
    "print(f\"Ground truth sentence:     {true_sentence}\")\n",
    "print(f\"Model translated sentence: {revised_response}\")\n",
    "print('---')\n",
    "print(f\"Ground truth sentence (cutted):     {ref}\")\n",
    "print(f\"Model translated sentence (cutted): {src}\")\n",
    "print('---')\n",
    "\n",
    "bleu_score = sentence_bleu([ref], src)\n",
    "print(f\"bleu_score: {bleu_score}\")\n",
    "print('---')\n",
    "\n",
    "gleu_score = gleu.sentence_gleu([ref], src, min_len=1, max_len=2)\n",
    "print(f\"gleu_score: {gleu_score}\")\n",
    "print('---')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
